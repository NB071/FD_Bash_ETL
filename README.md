<p align="center">
  <img src="https://nimabargestan.com/wp-content/uploads/2025/06/FD-Bash-Etl-1.png" alt="ETL Pipeline Diagram" width="500" height="500">
</p>

# ğŸ› ï¸ FD_Bash_ETL: Delivery Data Pipeline with AWS + Bash

This project implements a complete **ETL (Extract-Transform-Load)** pipeline in **Bash and AWK**, designed to clean and prepare delivery dataset files, convert them to JSON, and optionally visualize them in **Amazon QuickSight (QS)** using a manifest.

---

## ğŸ“¦ Project Overview

This project processes raw delivery data (`dataset.csv`) through:

1. **Extraction** from remote source (Kaggle)
2. **Transformation** into a clean, validated dataset
3. **Loading** to an AWS S3 bucket
4. **Visualization** via **Amazon QuickSight** with a prepared `manifest.json`

---

## ğŸ“ Project Structure

```text
FD_Bash_ETL/
â”œâ”€â”€ assets/                     # Supporting files
â”‚   â”œâ”€â”€ delivery_dashboard.pdf  # Exported QuickSight dashboard
â”‚   â”œâ”€â”€ manifest.json           # Used to load JSON into QuickSight
â”‚   â”œâ”€â”€ sample_output_console.png
â”‚   â””â”€â”€ sample_result_aws_s3.png
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/              # Cleaned output
â”‚   â”‚   â”œâ”€â”€ dataset.json
â”‚   â”‚   â””â”€â”€ dataset.csv
â”‚   â””â”€â”€ raw/                    # Original raw input
â”‚       â””â”€â”€ dataset.csv
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 1_extract.sh            # Downloads & unzips dataset
â”‚   â”œâ”€â”€ 2_transform.sh          # Cleans & validates data using AWK
â”‚   â”œâ”€â”€ 3_load.sh               # Uploads cleaned files to S3
â”‚   â””â”€â”€ main.sh                 # Runs full ETL pipeline
```

---

## ğŸ”§ Requirements

- `bash`
- `awk`
- `curl`
- `unzip`
- [`jq`](https://stedolan.github.io/jq/) (for JSON transformation)
- [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) (configured with `aws configure`)

---

## ğŸš€ How to Run

Run all stages in order with:

```bash
cd scripts && ./main.sh
```

## â˜ï¸ AWS Setup

- **ğŸª£ S3 Upload**

Ensure you've configured AWS CLI:

```bash
aws configure
```

Your cleaned dataset will be uploaded to:

- s3://`<your-bucket-name>`/`<your-bucket-path>`/dataset.csv
- s3://`<your-bucket-name>`/`<your-bucket-path>`/dataset.json

*where `<your-bucket-name>` & `<your-bucket-path>` are clearly indicated as constants defined inside **3_load.sh** along with other ones*

- **ğŸ“Š Amazon QuickSight**

To visualize the data in QuickSight:

1. Use `assets/manifest.json` for importing the JSON dataset.
2. In QuickSight, go to `New Dataset â†’ S3 â†’ Upload a manifest file`.
3. Ensure your S3 bucket *permissions* allow QuickSight to access the data.

---

## ğŸ–¼ï¸ Sample Visuals

Browse visual assets in the `assets/`folder:
- ğŸ“Š `delivery_dashboard.pdf`: Full dashboard export
- âœ… `sample_result_aws_s3.png`: AWS upload confirmation
- ğŸ§ª `sample_output_console.png`: CLI output of transformation stage

---

## ğŸ§¾ License

This project is licensed under the MIT License.
